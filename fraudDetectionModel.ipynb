{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa07e8ba-4df5-4305-9d03-b23326de2a58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, sum, avg, count, when, lit, hour\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcc701d5-7fee-4fc9-8576-68b846f1b94c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "database_name = \"banking_database\"\n",
    "customer_table_name = \"customers\"\n",
    "transactions_table_name = \"transactions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed2fd6a5-937b-4c7d-af29-8bcbceeb0667",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read existing Customer Data from the Delta Table\n",
    "try:\n",
    "    # Expected Customer Table Schema:\n",
    "    # - id (LongType)\n",
    "    # - first_name (StringType)\n",
    "    # - last_name (StringType)\n",
    "    # - age (IntegerType)\n",
    "    # - location (StringType)\n",
    "    # - annual_income (DoubleType)\n",
    "    # - dti (DoubleType)\n",
    "    # - ltv (DoubleType)\n",
    "    # - credit_score (IntegerType)\n",
    "    spark_customer_df = spark.read.format(\"delta\").table(f\"{database_name}.{customer_table_name}\")\n",
    "    print(f\"Customer table '{database_name}.{customer_table_name}' loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading customer table: {e}\")\n",
    "    print(\"Please ensure the database and table exist and are accessible with the correct schema.\")\n",
    "    raise \n",
    "\n",
    "# Read existing Transactions Data from the Delta Table\n",
    "try:\n",
    "    # Expected Transactions Table Schema:\n",
    "    # - customer_id (LongType)\n",
    "    # - transaction_date (TimestampType)\n",
    "    # - amount (DoubleType)\n",
    "    # - recipient (StringType)\n",
    "    # - device_type (StringType)\n",
    "    spark_transactions_df = spark.read.format(\"delta\").table(f\"{database_name}.{transactions_table_name}\")\n",
    "    print(f\"Transactions table '{database_name}.{transactions_table_name}' loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading transactions table: {e}\")\n",
    "    print(\"Please ensure the database and table exist and are accessible with the correct schema.\")\n",
    "    raise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4914ebd1-8e8a-453c-8991-0756d11e699c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join customer and transactions, and calculate aggregate features per customer and per transaction.\n",
    "\n",
    "spark_customer_df.createOrReplaceTempView(\"all_customers\")\n",
    "spark_transactions_df.createOrReplaceTempView(\"all_transactions\")\n",
    "print(\"\\nTemporary views 'all_customers' and 'all_transactions' created.\")\n",
    "\n",
    "features_df = spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        t.customer_id,\n",
    "        t.transaction_date,\n",
    "        t.amount,\n",
    "        t.recipient,\n",
    "        t.device_type,\n",
    "        c.age,\n",
    "        c.location,\n",
    "        c.annual_income,\n",
    "        c.dti,\n",
    "        c.ltv,\n",
    "        c.credit_score,\n",
    "        -- Global customer aggregates from transactions\n",
    "        SUM(t.amount) OVER (PARTITION BY t.customer_id) AS total_amount_per_customer,\n",
    "        AVG(t.amount) OVER (PARTITION BY t.customer_id) AS avg_amount_per_customer,\n",
    "        MAX(t.amount) OVER (PARTITION BY t.customer_id) AS max_amount_per_customer,\n",
    "        COUNT(1) OVER (PARTITION BY t.customer_id) AS num_transactions_per_customer,\n",
    "        -- Rolling window features for fraud detection (transaction-level)\n",
    "        COUNT(1) OVER (\n",
    "            PARTITION BY t.customer_id\n",
    "            ORDER BY t.transaction_date\n",
    "            RANGE BETWEEN INTERVAL 1 HOUR PRECEDING AND CURRENT ROW\n",
    "        ) AS transaction_count_last_hour,\n",
    "        SUM(t.amount) OVER (\n",
    "            PARTITION BY t.customer_id\n",
    "            ORDER BY t.transaction_date\n",
    "            RANGE BETWEEN INTERVAL 1 HOUR PRECEDING AND CURRENT ROW\n",
    "        ) AS transaction_sum_last_hour,\n",
    "        COUNT(1) OVER (\n",
    "            PARTITION BY t.customer_id\n",
    "            ORDER BY t.transaction_date\n",
    "            RANGE BETWEEN INTERVAL 24 HOURS PRECEDING AND CURRENT ROW\n",
    "        ) AS transaction_count_last_day,\n",
    "        SUM(t.amount) OVER (\n",
    "            PARTITION BY t.customer_id\n",
    "            ORDER BY t.transaction_date\n",
    "            RANGE BETWEEN INTERVAL 24 HOURS PRECEDING AND CURRENT ROW\n",
    "        ) AS transaction_sum_last_day,\n",
    "        HOUR(t.transaction_date) AS transaction_hour_of_day\n",
    "    FROM\n",
    "        all_transactions t\n",
    "    JOIN\n",
    "        all_customers c ON t.customer_id = c.id -- Join on customer_id from transactions and id from customers\n",
    "    ORDER BY\n",
    "        t.customer_id, t.transaction_date\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n--- Features DataFrame (first 10 rows) ---\")\n",
    "features_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afcf028c-3226-4d5e-aab3-40a33cc5f60c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create synthetic label for fraud detection\n",
    "\n",
    "# Define rules for synthetic label generation\n",
    "DUMMY_FRAUD_AMOUNT_THRESHOLD = 2000.0\n",
    "DUMMY_FRAUD_UNUSUAL_RECIPIENT = \"Unknown\"\n",
    "DUMMY_FRAUD_UNUSUAL_DEVICE = \"Unknown\"\n",
    "DUMMY_FRAUD_HIGH_FREQ_HOUR = 4 \n",
    "DUMMY_FRAUD_HIGH_SUM_HOUR = 2500.0 \n",
    "DUMMY_FRAUD_UNUSUAL_HOUR_START = 0 \n",
    "DUMMY_FRAUD_UNUSUAL_HOUR_END = 6 \n",
    "\n",
    "data_for_ml_df = features_df.withColumn(\n",
    "    \"label\", \n",
    "    when(col(\"amount\") >= DUMMY_FRAUD_AMOUNT_THRESHOLD, lit(1.0)) \n",
    "    .when(col(\"recipient\") == DUMMY_FRAUD_UNUSUAL_RECIPIENT, lit(1.0)) \n",
    "    .when(col(\"device_type\") == DUMMY_FRAUD_UNUSUAL_DEVICE, lit(1.0))\n",
    "    .when(col(\"transaction_count_last_hour\") >= DUMMY_FRAUD_HIGH_FREQ_HOUR, lit(1.0)) \n",
    "    .when(col(\"transaction_sum_last_hour\") >= DUMMY_FRAUD_HIGH_SUM_HOUR, lit(1.0)) \n",
    "    .when((col(\"transaction_hour_of_day\") >= DUMMY_FRAUD_UNUSUAL_HOUR_START) & \\\n",
    "          (col(\"transaction_hour_of_day\") < DUMMY_FRAUD_UNUSUAL_HOUR_END) & \\\n",
    "          (col(\"amount\") > 1000), lit(1.0)) \n",
    "    .otherwise(lit(0.0)) \n",
    ")\n",
    "\n",
    "print(\"\\n--- Data with Dummy 'label' for ML Training (first 10 rows) ---\")\n",
    "data_for_ml_df.select(\n",
    "    \"customer_id\", \"transaction_date\", \"amount\", \"recipient\", \"device_type\", \"label\"\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecca6e24-64d1-4872-9c6e-d267acbe88da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define numerical and categorical features for the ML model\n",
    "numerical_features = [\n",
    "    \"amount\", \"age\", \"annual_income\", \"dti\", \"ltv\", \"credit_score\",\n",
    "    \"total_amount_per_customer\", \"avg_amount_per_customer\", \"max_amount_per_customer\",\n",
    "    \"num_transactions_per_customer\", \"transaction_count_last_hour\", \"transaction_sum_last_hour\",\n",
    "    \"transaction_count_last_day\", \"transaction_sum_last_day\", \"transaction_hour_of_day\"\n",
    "]\n",
    "categorical_features = [\"location\", \"recipient\", \"device_type\"]\n",
    "\n",
    "# Handle potential nulls in numerical features by filling with 0 or mean\n",
    "\n",
    "for col_name in numerical_features:\n",
    "    data_for_ml_df = data_for_ml_df.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
    "\n",
    "    data_for_ml_df = data_for_ml_df.fillna(0, subset=[col_name])\n",
    "\n",
    "# StringIndexer for categorical features\n",
    "indexers = []\n",
    "for col_name in categorical_features:\n",
    "    indexer = StringIndexer()\n",
    "    indexer.setInputCol(col_name)\n",
    "    indexer.setOutputCol(col_name + \"_indexed\")\n",
    "    indexer.setHandleInvalid(\"keep\") \n",
    "    indexers.append(indexer)\n",
    "\n",
    "# OneHotEncoder for indexed categorical features\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=indexer.getOutputCol() + \"_encoded\")\n",
    "    for indexer in indexers\n",
    "]\n",
    "\n",
    "# Assemble all features into a single vector column\n",
    "assembler_inputs = numerical_features + [encoder.getOutputCol() for encoder in encoders]\n",
    "vector_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\", handleInvalid=\"keep\")\n",
    "\n",
    "# Create a Pipeline for feature transformation\n",
    "feature_pipeline = Pipeline(stages=indexers + encoders + [vector_assembler])\n",
    "\n",
    "# Fit the feature pipeline to the data and transform it\n",
    "\n",
    "ml_data = feature_pipeline.fit(data_for_ml_df).transform(data_for_ml_df)\n",
    "\n",
    "print(\"\\n--- ML Data with 'features' Vector Column (first 5 rows) ---\")\n",
    "ml_data.select(\"customer_id\", \"amount\", \"label\", \"features\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55aa0743-ec71-4fbb-a128-fb96ec5b0d04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train and Evaluate Model\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_data, test_data = ml_data.randomSplit([0.7, 0.3], seed=42)\n",
    "print(f\"\\nTraining data count: {train_data.count()}\")\n",
    "print(f\"Test data count: {test_data.count()}\")\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10, regParam=0.1, elasticNetParam=0.0)\n",
    "\n",
    "# Train the model\n",
    "print(\"\\n--- Training Logistic Regression Model ---\")\n",
    "lr_model = lr.fit(train_data)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "predictions.select(\"customer_id\", \"amount\", \"label\", \"prediction\", \"probability\").show(10, truncate=False)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(f\"\\nArea Under ROC (AUC) on test data: {auc}\")\n",
    "\n",
    "evaluator_accuracy = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"accuracy\")\n",
    "accuracy = evaluator_accuracy.evaluate(predictions)\n",
    "print(f\"Accuracy on test data: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "540ea28e-7e3c-46da-a9dc-2be00463bf5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply the Trained Model for Fraud Prediction\n",
    "\n",
    "final_predictions_df = lr_model.transform(ml_data)\n",
    "\n",
    "# Select relevant columns and rename 'prediction' to 'is_fraudulent' for clarity\n",
    "fraud_results_df = final_predictions_df.select(\n",
    "    col(\"customer_id\"),\n",
    "    col(\"transaction_date\"),\n",
    "    col(\"amount\"),\n",
    "    col(\"recipient\"),\n",
    "    col(\"device_type\"),\n",
    "    col(\"label\").alias(\"actual_fraud_label\"), \n",
    "    col(\"prediction\").cast(\"boolean\").alias(\"is_fraudulent_predicted\"), \n",
    "    col(\"probability\")[1].alias(\"fraud_probability\") \n",
    ")\n",
    "\n",
    "print(\"\\n--- Final Fraud Predictions on All Data (ordered by fraud probability) ---\")\n",
    "fraud_results_df.orderBy(col(\"fraud_probability\").desc()).show(truncate=False)\n",
    "\n",
    "print(\"\\n--- Transaction Records Most Susceptible to Fraud (Predicted by ML Model) ---\")\n",
    "# Filter for predicted fraudulent transactions and order by probability to see most susceptible\n",
    "susceptible_transactions_df = fraud_results_df.filter(col(\"is_fraudulent_predicted\") == True).orderBy(col(\"fraud_probability\").desc())\n",
    "susceptible_transactions_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d2fab7e-0a4f-4116-99a5-bbb14041edca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ml_fraud_output_table_name = \"ml_susceptible_fraud_transactions\"\n",
    "susceptible_transactions_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{database_name}.{ml_fraud_output_table_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "fraudDetectionModel",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
