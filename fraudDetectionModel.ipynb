{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad18ae02-0f2a-4b99-85b3-0683cf1385d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fraud Detection Model that will scan through transaction records and identify suspicious transactions\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, sum, avg, count, when, lit, current_timestamp, date_format, hour, dayofmonth\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# --- 1. Define Database and Table Names, and Read Existing Data ---\n",
    "# IMPORTANT: Ensure these database and table names match your actual setup in Databricks.\n",
    "# The 'customers' and 'transactions' tables are assumed to already exist with the specified schema.\n",
    "\n",
    "database_name = \"fraud_detection_db\" # Example: \"your_data_warehouse\"\n",
    "customer_table_name = \"customers\"\n",
    "transactions_table_name = \"transactions\"\n",
    "\n",
    "# Read existing Customer Data from the Delta Table\n",
    "try:\n",
    "    # Expected Customer Table Schema:\n",
    "    # - Id (LongType)\n",
    "    # - First Name (StringType)\n",
    "    # - Last Name (StringType)\n",
    "    # - Age (IntegerType)\n",
    "    # - Location (StringType)\n",
    "    # - Annual Income (DoubleType)\n",
    "    # - Debt-To-Income Ratio (DTI) (DoubleType)\n",
    "    # - Loan-to-Value Ratio (LTV) (DoubleType)\n",
    "    # - Average Monthly Spending (DoubleType)\n",
    "    # - Credit Score (IntegerType)\n",
    "    spark_customer_df = spark.read.format(\"delta\").table(f\"{database_name}.{customer_table_name}\")\n",
    "    print(f\"Customer table '{database_name}.{customer_table_name}' loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading customer table: {e}\")\n",
    "    print(\"Please ensure the database and table exist and are accessible with the correct schema.\")\n",
    "    raise # Re-raise the exception to stop execution if tables are not found\n",
    "\n",
    "# Read existing Transactions Data from the Delta Table\n",
    "try:\n",
    "    # Expected Transactions Table Schema:\n",
    "    # - Customer ID (LongType)\n",
    "    # - Transaction Date (TimestampType)\n",
    "    # - Amount (DoubleType)\n",
    "    # - Recipient (StringType)\n",
    "    # - Device Type (StringType)\n",
    "    spark_transactions_df = spark.read.format(\"delta\").table(f\"{database_name}.{transactions_table_name}\")\n",
    "    print(f\"Transactions table '{database_name}.{transactions_table_name}' loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading transactions table: {e}\")\n",
    "    print(\"Please ensure the database and table exist and are accessible with the correct schema.\")\n",
    "    raise # Re-raise the exception to stop execution if tables are not found\n",
    "\n",
    "print(\"\\n--- Raw Customer Data (first 5 rows) ---\")\n",
    "spark_customer_df.show(5)\n",
    "\n",
    "print(\"\\n--- Raw Transactions Data (first 5 rows) ---\")\n",
    "spark_transactions_df.show(5, truncate=False)\n",
    "\n",
    "# --- 2. Feature Engineering using Spark SQL ---\n",
    "# Join customer and transactions, and calculate aggregate features per customer and per transaction.\n",
    "\n",
    "# Define temporary views for easier SQL querying\n",
    "spark_customer_df.createOrReplaceTempView(\"all_customers\")\n",
    "spark_transactions_df.createOrReplaceTempView(\"all_transactions\")\n",
    "print(\"\\nTemporary views 'all_customers' and 'all_transactions' created.\")\n",
    "\n",
    "# SQL query to join tables and calculate features based on the new schema\n",
    "# This query now includes features relevant for ML models.\n",
    "features_df = spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        t.\"Customer ID\", -- Using \"Customer ID\" from transactions\n",
    "        c.Id AS customer_id_from_customer_table, -- Using Id from customers for join clarity\n",
    "        c.\"First Name\" AS customer_first_name,\n",
    "        c.\"Last Name\" AS customer_last_name,\n",
    "        t.Amount,\n",
    "        t.\"Transaction Date\" AS transaction_timestamp, -- Alias for consistency with previous logic\n",
    "        t.Recipient,\n",
    "        t.\"Device Type\",\n",
    "        c.Age,\n",
    "        c.Location,\n",
    "        c.\"Annual Income\",\n",
    "        c.\"Debt-To-Income Ratio (DTI)\" AS dti,\n",
    "        c.\"Loan-to-Value Ratio (LTV)\" AS ltv,\n",
    "        c.\"Average Monthly Spending\" AS avg_monthly_spending,\n",
    "        c.\"Credit Score\" AS credit_score,\n",
    "        -- Global customer aggregates (using \"Customer ID\" from transactions for partitioning)\n",
    "        SUM(t.Amount) OVER (PARTITION BY t.\"Customer ID\") AS total_amount_per_customer,\n",
    "        AVG(t.Amount) OVER (PARTITION BY t.\"Customer ID\") AS avg_amount_per_customer,\n",
    "        MAX(t.Amount) OVER (PARTITION BY t.\"Customer ID\") AS max_amount_per_customer,\n",
    "        COUNT(1) OVER (PARTITION BY t.\"Customer ID\") AS num_transactions_per_customer, -- Count rows\n",
    "        -- Rolling window features for fraud detection\n",
    "        COUNT(1) OVER (\n",
    "            PARTITION BY t.\"Customer ID\"\n",
    "            ORDER BY t.\"Transaction Date\"\n",
    "            RANGE BETWEEN INTERVAL 1 HOUR PRECEDING AND CURRENT ROW\n",
    "        ) AS transaction_count_last_hour,\n",
    "        SUM(t.Amount) OVER (\n",
    "            PARTITION BY t.\"Customer ID\"\n",
    "            ORDER BY t.\"Transaction Date\"\n",
    "            RANGE BETWEEN INTERVAL 1 HOUR PRECEDING AND CURRENT ROW\n",
    "        ) AS transaction_sum_last_hour,\n",
    "        COUNT(1) OVER (\n",
    "            PARTITION BY t.\"Customer ID\"\n",
    "            ORDER BY t.\"Transaction Date\"\n",
    "            RANGE BETWEEN INTERVAL 24 HOURS PRECEDING AND CURRENT ROW\n",
    "        ) AS transaction_count_last_day,\n",
    "        SUM(t.Amount) OVER (\n",
    "            PARTITION BY t.\"Customer ID\"\n",
    "            ORDER BY t.\"Transaction Date\"\n",
    "            RANGE BETWEEN INTERVAL 24 HOURS PRECEDING AND CURRENT ROW\n",
    "        ) AS transaction_sum_last_day,\n",
    "        HOUR(t.\"Transaction Date\") AS transaction_hour_of_day\n",
    "    FROM\n",
    "        all_transactions t\n",
    "    JOIN\n",
    "        all_customers c ON t.\"Customer ID\" = c.Id -- Join on Customer ID and Id\n",
    "    ORDER BY\n",
    "        t.\"Customer ID\", t.\"Transaction Date\"\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n--- Features DataFrame (first 10 rows) ---\")\n",
    "features_df.show(truncate=False)\n",
    "\n",
    "# --- 3. Prepare Data for Machine Learning Model ---\n",
    "# For demonstration, we'll create a dummy 'is_fraudulent' label based on some rules.\n",
    "# In a real scenario, this label would come from historical fraud data.\n",
    "\n",
    "# Apply the same rule-based logic to create a 'label' column for training\n",
    "# This is a placeholder for actual historical fraud labels.\n",
    "data_for_ml_df = features_df.withColumn(\n",
    "    \"label\", # 'label' is the standard column name for target variable in Spark ML\n",
    "    when(col(\"Amount\") >= 1500.00, lit(1.0)) # Very high value transaction\n",
    "    .when(col(\"transaction_count_last_hour\") >= 4, lit(1.0)) # More than 4 transactions in an hour\n",
    "    .when(col(\"transaction_sum_last_hour\") >= 2000.00, lit(1.0)) # Sum of transactions over $2000 in an hour\n",
    "    .when((col(\"transaction_hour_of_day\") >= UNUSUAL_HOUR_START) & (col(\"transaction_hour_of_day\") < UNUSUAL_HOUR_END) & (col(\"Amount\") > 800), lit(1.0)) # High value at unusual hour\n",
    "    .otherwise(lit(0.0)) # Not fraudulent\n",
    ")\n",
    "\n",
    "print(\"\\n--- Data with Dummy 'label' for ML Training ---\")\n",
    "data_for_ml_df.select(\"Customer ID\", \"Amount\", \"transaction_count_last_hour\", \"transaction_sum_last_hour\", \"transaction_hour_of_day\", \"label\").show(truncate=False)\n",
    "\n",
    "# Define numerical and categorical features\n",
    "numerical_features = [\n",
    "    \"Amount\", \"Age\", \"Annual Income\", \"dti\", \"ltv\", \"avg_monthly_spending\", \"credit_score\",\n",
    "    \"total_amount_per_customer\", \"avg_amount_per_customer\", \"max_amount_per_customer\",\n",
    "    \"num_transactions_per_customer\", \"transaction_count_last_hour\", \"transaction_sum_last_hour\",\n",
    "    \"transaction_count_last_day\", \"transaction_sum_last_day\", \"transaction_hour_of_day\"\n",
    "]\n",
    "categorical_features = [\"Location\", \"Device Type\", \"Recipient\"] # Note: Recipient might have high cardinality\n",
    "\n",
    "# Create a Pipeline for feature processing\n",
    "# StringIndexer for categorical features\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=col, outputCol=col + \"_indexed\", handleInvalid=\"keep\")\n",
    "    for col in categorical_features\n",
    "]\n",
    "\n",
    "# OneHotEncoder for indexed categorical features\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=indexer.getOutputCol() + \"_encoded\")\n",
    "    for indexer in indexers\n",
    "]\n",
    "\n",
    "# Assemble all features into a single vector column\n",
    "assembler_inputs = numerical_features + [encoder.getOutputCol() for encoder in encoders]\n",
    "vector_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\", handleInvalid=\"keep\")\n",
    "\n",
    "# Create a Pipeline for feature transformation\n",
    "feature_pipeline = Pipeline(stages=indexers + encoders + [vector_assembler])\n",
    "\n",
    "# Fit the feature pipeline to the data and transform it\n",
    "# This step creates the 'features' column required by ML models\n",
    "ml_data = feature_pipeline.fit(data_for_ml_df).transform(data_for_ml_df)\n",
    "\n",
    "print(\"\\n--- ML Data with 'features' Vector Column (first 5 rows) ---\")\n",
    "ml_data.select(\"Customer ID\", \"Amount\", \"label\", \"features\").show(5, truncate=False)\n",
    "\n",
    "# --- 4. Train and Evaluate Machine Learning Model ---\n",
    "\n",
    "# Split data into training and test sets\n",
    "# In a real-world scenario, you might split by time to avoid data leakage.\n",
    "train_data, test_data = ml_data.randomSplit([0.7, 0.3], seed=42)\n",
    "print(f\"\\nTraining data count: {train_data.count()}\")\n",
    "print(f\"Test data count: {test_data.count()}\")\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Train the model\n",
    "print(\"\\n--- Training Logistic Regression Model ---\")\n",
    "lr_model = lr.fit(train_data)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "print(\"\\n--- Predictions on Test Data (first 10 rows) ---\")\n",
    "predictions.select(\"Customer ID\", \"Amount\", \"label\", \"prediction\", \"probability\").show(10, truncate=False)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(f\"\\nArea Under ROC (AUC) on test data: {auc}\")\n",
    "\n",
    "evaluator_accuracy = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"accuracy\")\n",
    "accuracy = evaluator_accuracy.evaluate(predictions)\n",
    "print(f\"Accuracy on test data: {accuracy}\")\n",
    "\n",
    "# --- 5. Apply the Trained Model for Fraud Prediction ---\n",
    "# Apply the trained model to the entire dataset (or new incoming data)\n",
    "\n",
    "# First, re-apply the feature transformation to the full dataset if not already done\n",
    "# (ml_data already has features, but if you were processing new data, you'd do this)\n",
    "final_predictions_df = lr_model.transform(ml_data)\n",
    "\n",
    "# Select relevant columns and rename 'prediction' to 'is_fraudulent' for clarity\n",
    "fraud_results_df = final_predictions_df.select(\n",
    "    col(\"Customer ID\"),\n",
    "    col(\"Amount\"),\n",
    "    col(\"transaction_timestamp\"),\n",
    "    col(\"Recipient\"),\n",
    "    col(\"Device Type\"),\n",
    "    col(\"label\").alias(\"actual_fraud_label\"), # The dummy label used for training\n",
    "    col(\"prediction\").cast(\"boolean\").alias(\"is_fraudulent_predicted\"), # Convert 0.0/1.0 to boolean\n",
    "    col(\"probability\")[1].alias(\"fraud_probability\") # Probability of being class 1 (fraud)\n",
    ")\n",
    "\n",
    "print(\"\\n--- Final Fraud Predictions on All Data ---\")\n",
    "fraud_results_df.orderBy(col(\"fraud_probability\").desc()).show(truncate=False)\n",
    "\n",
    "print(\"\\n--- Potentially Fraudulent Transactions (Predicted by ML Model) ---\")\n",
    "fraud_results_df.filter(col(\"is_fraudulent_predicted\") == True).orderBy(col(\"fraud_probability\").desc()).show(truncate=False)\n",
    "\n",
    "# --- Optional: Save Predicted Fraudulent Transactions to a separate table ---\n",
    "ml_fraud_output_table_name = \"ml_potential_fraud_transactions\"\n",
    "fraud_results_df.filter(col(\"is_fraudulent_predicted\") == True) \\\n",
    "    .write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{database_name}.{ml_fraud_output_table_name}\")\n",
    "\n",
    "print(f\"\\nML-predicted fraudulent transactions saved to '{database_name}.{ml_fraud_output_table_name}'.\")\n",
    "\n",
    "print(\"\\n--- Verify ML Fraud Output Table ---\")\n",
    "spark.sql(f\"SELECT * FROM {database_name}.{ml_potential_fraud_transactions}\").show(truncate=False)\n",
    "\n",
    "# --- Clean up temporary views (optional) ---\n",
    "spark.sql(\"DROP VIEW IF EXISTS all_transactions\")\n",
    "spark.sql(\"DROP VIEW IF EXISTS all_customers\")\n",
    "print(\"\\nTemporary views dropped.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "fraudDetectionModel",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
