{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc887f65-0efe-4990-a533-8d492e2d60e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clustering Model to segment customers\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd # Still imported, but not used for data creation in this version\n",
    "from pyspark.sql.functions import col, when, lit, avg, count, sum\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import DoubleType # Import DoubleType for casting if needed\n",
    "\n",
    "# --- 1. Define Database and Table Names, and Read Existing Customer Data ---\n",
    "# IMPORTANT: Ensure these database and table names match your actual setup in Databricks.\n",
    "# The 'customers' table is assumed to already exist with the specified schema.\n",
    "\n",
    "database_name = \"fraud_detection_db\" # Example: \"your_data_warehouse\"\n",
    "customer_table_name = \"customers\"\n",
    "\n",
    "# Read existing Customer Data from the Delta Table\n",
    "try:\n",
    "    # Expected Customer Table Schema:\n",
    "    # - Id (LongType)\n",
    "    # - First Name (StringType)\n",
    "    # - Last Name (StringType)\n",
    "    # - Age (IntegerType)\n",
    "    # - Location (StringType)\n",
    "    # - Annual Income (DoubleType)\n",
    "    # - Debt-To-Income Ratio (DTI) (DoubleType)\n",
    "    # - Loan-to-Value Ratio (LTV) (DoubleType)\n",
    "    # - Average Monthly Spending (DoubleType)\n",
    "    # - Credit Score (IntegerType)\n",
    "    spark_customer_df = spark.read.format(\"delta\").table(f\"{database_name}.{customer_table_name}\")\n",
    "    print(f\"Customer table '{database_name}.{customer_table_name}' loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading customer table: {e}\")\n",
    "    print(\"Please ensure the database and table exist and are accessible with the correct schema.\")\n",
    "    raise # Re-raise the exception to stop execution if tables are not found\n",
    "\n",
    "print(\"\\n--- Raw Customer Data (first 5 rows) ---\")\n",
    "spark_customer_df.show(5)\n",
    "\n",
    "# --- 2. Prepare Data for K-Means Clustering ---\n",
    "# We will focus on 'Annual Income' and 'Average Monthly Spending' for segmentation.\n",
    "\n",
    "print(\"\\n--- Starting Customer Segmentation (K-Means Clustering) ---\")\n",
    "\n",
    "# Select relevant features for clustering: Annual Income and Average Monthly Spending\n",
    "# Ensure these columns are of numeric type for clustering and handle any potential nulls.\n",
    "clustering_data_df = spark_customer_df.select(\n",
    "    \"Id\",\n",
    "    col(\"Annual Income\").cast(DoubleType()).alias(\"Annual_Income\"),\n",
    "    col(\"Average Monthly Spending\").cast(DoubleType()).alias(\"Average_Monthly_Spending\")\n",
    ").na.drop(subset=[\"Annual_Income\", \"Average_Monthly_Spending\"]) # Drop rows with nulls in these key columns\n",
    "\n",
    "print(\"\\n--- Data for Clustering (first 5 rows) ---\")\n",
    "clustering_data_df.show(5)\n",
    "\n",
    "# Assemble features into a single vector. This is a required step for Spark ML models.\n",
    "clustering_assembler = VectorAssembler(\n",
    "    inputCols=[\"Annual_Income\", \"Average_Monthly_Spending\"],\n",
    "    outputCol=\"features_clustering\",\n",
    "    handleInvalid=\"skip\" # Skip rows with invalid (e.g., non-numeric) feature values\n",
    ")\n",
    "\n",
    "# It's crucial to scale features for K-Means to ensure both 'Annual_Income' and\n",
    "# 'Average_Monthly_Spending' contribute equally to the distance calculations.\n",
    "# StandardScaler scales features to have zero mean and unit variance.\n",
    "scaler = StandardScaler(inputCol=\"features_clustering\", outputCol=\"scaled_features_clustering\",\n",
    "                        withStd=True, withMean=False) # withMean=False for sparse vectors\n",
    "\n",
    "# --- 3. Train K-Means Clustering Model ---\n",
    "# Define the K-Means model with k=4 categories as requested.\n",
    "kmeans = KMeans(featuresCol=\"scaled_features_clustering\", k=4, seed=42)\n",
    "\n",
    "# Create a pipeline for clustering: (assembler -> scaler -> kmeans)\n",
    "# This pipeline will first assemble the features, then scale them, and finally apply K-Means.\n",
    "clustering_pipeline = Pipeline(stages=[clustering_assembler, scaler, kmeans])\n",
    "\n",
    "# Train the K-Means model on the prepared data.\n",
    "print(\"\\n--- Training K-Means Clustering Model ---\")\n",
    "kmeans_model = clustering_pipeline.fit(clustering_data_df)\n",
    "print(\"K-Means Model training complete.\")\n",
    "\n",
    "# --- 4. Assign Clusters and Interpret Categories ---\n",
    "# Make predictions (assign each customer to a cluster)\n",
    "clustered_customers_df = kmeans_model.transform(clustering_data_df)\n",
    "\n",
    "print(\"\\n--- Customers with Cluster Assignments (first 10 rows) ---\")\n",
    "clustered_customers_df.select(\"Id\", \"Annual_Income\", \"Average_Monthly_Spending\", \"prediction\").show(10)\n",
    "\n",
    "# To interpret the clusters, we'll calculate the mean of 'Annual_Income' and 'Average_Monthly_Spending'\n",
    "# for each cluster. This helps us understand what each cluster represents.\n",
    "print(\"\\n--- Average Income and Spending per Cluster (for interpretation) ---\")\n",
    "cluster_summary = clustered_customers_df.groupBy(\"prediction\").agg(\n",
    "    avg(\"Annual_Income\").alias(\"Avg_Annual_Income\"),\n",
    "    avg(\"Average_Monthly_Spending\").alias(\"Avg_Monthly_Spending\")\n",
    ").orderBy(\"prediction\")\n",
    "cluster_summary.show()\n",
    "\n",
    "# --- Assign Human-Readable Categories based on Cluster Summary ---\n",
    "# Based on the `cluster_summary` output, you would manually map the cluster IDs\n",
    "# (0, 1, 2, 3) to the desired categories:\n",
    "# - High Income High Spenders\n",
    "# - High Income Low Spenders\n",
    "# - Low Income High Spenders\n",
    "# - Low Income Low Spenders\n",
    "\n",
    "# IMPORTANT: The actual mapping from 'prediction' (cluster ID) to category\n",
    "# depends entirely on the `cluster_summary` output for your specific data.\n",
    "# You need to run the code, look at `cluster_summary.show()`, and then\n",
    "# define the conditions below.\n",
    "\n",
    "# Example of how you might map based on the cluster summary.\n",
    "# This is a placeholder. YOU MUST ADJUST THIS LOGIC BASED ON YOUR DATA'S CLUSTER_SUMMARY.\n",
    "# For instance, if cluster 0 has high average income and high average spending, map it to \"High Income High Spenders\".\n",
    "\n",
    "# As a generic illustrative example (you'll replace this with specific cluster IDs):\n",
    "# Let's assume you determine thresholds based on the overall data distribution\n",
    "# or by inspecting the cluster centroids.\n",
    "overall_avg_income = clustered_customers_df.agg(avg(\"Annual_Income\")).collect()[0][0]\n",
    "overall_avg_spending = clustered_customers_df.agg(avg(\"Average_Monthly_Spending\")).collect()[0][0]\n",
    "\n",
    "print(f\"\\nOverall Average Annual Income: {overall_avg_income:.2f}\")\n",
    "print(f\"Overall Average Monthly Spending: {overall_avg_spending:.2f}\")\n",
    "\n",
    "customer_segments_df = clustered_customers_df.withColumn(\n",
    "    \"customer_category\",\n",
    "    when((col(\"Annual_Income\") >= overall_avg_income) & (col(\"Average_Monthly_Spending\") >= overall_avg_spending), lit(\"High Income High Spenders\"))\n",
    "    .when((col(\"Annual_Income\") >= overall_avg_income) & (col(\"Average_Monthly_Spending\") < overall_avg_spending), lit(\"High Income Low Spenders\"))\n",
    "    .when((col(\"Annual_Income\") < overall_avg_income) & (col(\"Average_Monthly_Spending\") >= overall_avg_spending), lit(\"Low Income High Spenders\"))\n",
    "    .otherwise(lit(\"Low Income Low Spenders\"))\n",
    ")\n",
    "\n",
    "# Alternatively, if you have a clear mapping from 'prediction' to category after reviewing `cluster_summary`:\n",
    "# customer_segments_df = clustered_customers_df.withColumn(\n",
    "#     \"customer_category\",\n",
    "#     when(col(\"prediction\") == 0, lit(\"High Income High Spenders\")) # Example: if cluster 0 is the high-high group\n",
    "#     .when(col(\"prediction\") == 1, lit(\"Low Income Low Spenders\"))  # Example: if cluster 1 is the low-low group\n",
    "#     .when(col(\"prediction\") == 2, lit(\"High Income Low Spenders\")) # Example: if cluster 2 is the high-low group\n",
    "#     .when(col(\"prediction\") == 3, lit(\"Low Income High Spenders\")) # Example: if cluster 3 is the low-high group\n",
    "#     .otherwise(lit(\"Uncategorized\")) # Fallback for any unmapped clusters\n",
    "# )\n",
    "\n",
    "print(\"\\n--- Customers with Assigned Categories (first 10 rows) ---\")\n",
    "customer_segments_df.select(\"Id\", \"Annual_Income\", \"Average_Monthly_Spending\", \"prediction\", \"customer_category\").show(10, truncate=False)\n",
    "\n",
    "# --- 5. Save Customer Segmentation Results to a New Table ---\n",
    "customer_segmentation_output_table_name = \"customer_income_spending_segments\"\n",
    "customer_segments_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{database_name}.{customer_segmentation_output_table_name}\")\n",
    "\n",
    "print(f\"\\nCustomer segmentation results saved to '{database_name}.{customer_segmentation_output_table_name}'.\")\n",
    "\n",
    "print(\"\\n--- Verify Customer Segmentation Output Table ---\")\n",
    "spark.sql(f\"SELECT * FROM {database_name}.{customer_segmentation_output_table_name}\").show(truncate=False)\n",
    "\n",
    "# --- Clean up temporary views (optional) ---\n",
    "# It's good practice to drop temporary views if they are no longer needed.\n",
    "spark.sql(\"DROP VIEW IF EXISTS all_customers\")\n",
    "# The transactions table was not directly used in this model, so no view for it.\n",
    "print(\"\\nTemporary views dropped.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "customerSegmentationModel",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
