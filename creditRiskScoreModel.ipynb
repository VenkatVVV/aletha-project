{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "111e7d6b-e666-4530-a842-7f9297c8ba7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, sum, avg, count, when, lit, hour, current_timestamp\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.types import DoubleType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87ee994f-1401-4662-82f7-ccca0bcd0b40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "database_name = \"banking_database\"\n",
    "customer_table_name = \"customers\"\n",
    "transactions_table_name = \"transactions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a91f6885-b1db-462d-8c8c-a8fa56fc2c41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Read existing Customer Data from the Delta Table\n",
    "try:\n",
    "    # Expected Customer Table Schema:\n",
    "    # - id (LongType)\n",
    "    # - first_name (StringType)\n",
    "    # - last_name (StringType)\n",
    "    # - age (IntegerType)\n",
    "    # - location (StringType)\n",
    "    # - annual_income (DoubleType)\n",
    "    # - dti (DoubleType)\n",
    "    # - ltv (DoubleType)\n",
    "    # - credit_score (IntegerType)\n",
    "    spark_customer_df = spark.read.format(\"delta\").table(f\"{database_name}.{customer_table_name}\")\n",
    "    print(f\"Customer table '{database_name}.{customer_table_name}' loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading customer table: {e}\")\n",
    "    raise \n",
    "\n",
    "# Read existing Transactions Data from the Delta Table\n",
    "try:\n",
    "    # Expected Transactions Table Schema:\n",
    "    # - customer_id (LongType)\n",
    "    # - transaction_date (TimestampType)\n",
    "    # - amount (DoubleType)\n",
    "    # - recipient (StringType)\n",
    "    # - device_type (StringType)\n",
    "    spark_transactions_df = spark.read.format(\"delta\").table(f\"{database_name}.{transactions_table_name}\")\n",
    "    print(f\"Transactions table '{database_name}.{transactions_table_name}' loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading transactions table: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52d71cba-aa63-41fc-b868-bd984cedfc8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Feature Engineering and Data Preparation\n",
    "\n",
    "# Define temporary views for easier SQL querying\n",
    "spark_customer_df.createOrReplaceTempView(\"all_customers\")\n",
    "if spark_transactions_df:\n",
    "    spark_transactions_df.createOrReplaceTempView(\"all_transactions\")\n",
    "    print(\"\\nTemporary views 'all_customers' and 'all_transactions' created.\")\n",
    "else:\n",
    "    print(\"\\nTemporary view 'all_customers' created. 'all_transactions' not created due to missing data.\")\n",
    "\n",
    "\n",
    "# Aggregate transaction data to customer level (if transactions table exists)\n",
    "if spark_transactions_df:\n",
    "    customer_transaction_aggregates = spark.sql(\"\"\"\n",
    "        SELECT\n",
    "            customer_id,\n",
    "            COUNT(DISTINCT transaction_date) AS num_transaction_days,\n",
    "            COUNT(1) AS total_transactions,\n",
    "            SUM(amount) AS total_spending,\n",
    "            AVG(amount) AS avg_transaction_amount,\n",
    "            MAX(amount) AS max_transaction_amount,\n",
    "            COUNT(DISTINCT recipient) AS num_unique_recipients,\n",
    "            COUNT(DISTINCT device_type) AS num_unique_devices,\n",
    "            SUM(CASE WHEN transaction_date >= date_sub(current_date(), 30) THEN amount ELSE 0 END) AS spending_last_30_days,\n",
    "            COUNT(CASE WHEN transaction_date >= date_sub(current_date(), 30) THEN 1 ELSE NULL END) AS transactions_last_30_days\n",
    "        FROM\n",
    "            all_transactions\n",
    "        GROUP BY\n",
    "            customer_id\n",
    "    \"\"\")\n",
    "    customer_transaction_aggregates.createOrReplaceTempView(\"customer_tx_aggregates\")\n",
    "    print(\"\\nCustomer transaction aggregates created.\")\n",
    "else:\n",
    "    # Create an empty DataFrame with the same schema if transactions table is missing\n",
    "    from pyspark.sql.types import StructType, StructField, LongType, DoubleType\n",
    "    customer_transaction_aggregates_schema = StructType([\n",
    "        StructField(\"customer_id\", LongType(), True),\n",
    "        StructField(\"num_transaction_days\", LongType(), True),\n",
    "        StructField(\"total_transactions\", LongType(), True),\n",
    "        StructField(\"total_spending\", DoubleType(), True),\n",
    "        StructField(\"avg_transaction_amount\", DoubleType(), True),\n",
    "        StructField(\"max_transaction_amount\", DoubleType(), True),\n",
    "        StructField(\"num_unique_recipients\", LongType(), True),\n",
    "        StructField(\"num_unique_devices\", LongType(), True),\n",
    "        StructField(\"spending_last_30_days\", DoubleType(), True),\n",
    "        StructField(\"transactions_last_30_days\", LongType(), True)\n",
    "    ])\n",
    "    customer_transaction_aggregates = spark.createDataFrame([], schema=customer_transaction_aggregates_schema)\n",
    "    customer_transaction_aggregates.createOrReplaceTempView(\"customer_tx_aggregates\")\n",
    "    print(\"\\nEmpty customer transaction aggregates created due to missing transactions table.\")\n",
    "\n",
    "\n",
    "# Join customer data with aggregated transaction data\n",
    "# The target variable for regression is 'credit_score'\n",
    "customer_features_df = spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        c.id AS customer_id,\n",
    "        c.age,\n",
    "        c.location,\n",
    "        c.annual_income,\n",
    "        c.dti,\n",
    "        c.ltv,\n",
    "        c.credit_score AS label,\n",
    "        t.num_transaction_days,\n",
    "        t.total_transactions,\n",
    "        t.total_spending,\n",
    "        t.avg_transaction_amount,\n",
    "        t.max_transaction_amount,\n",
    "        t.num_unique_recipients,\n",
    "        t.num_unique_devices,\n",
    "        t.spending_last_30_days,\n",
    "        t.transactions_last_30_days\n",
    "    FROM\n",
    "        all_customers c\n",
    "    LEFT JOIN \n",
    "        customer_tx_aggregates t ON c.id = t.customer_id\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "defc43da-6b91-4406-b5cd-ceb584ab7350",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prepare Features for Model\n",
    "\n",
    "# Define numerical and categorical features\n",
    "\n",
    "numerical_features = [\n",
    "    \"age\", \"annual_income\", \"dti\", \"ltv\",\n",
    "    \"num_transaction_days\", \"total_transactions\", \"total_spending\",\n",
    "    \"avg_transaction_amount\", \"max_transaction_amount\", \"num_unique_recipients\",\n",
    "    \"num_unique_devices\", \"spending_last_30_days\", \"transactions_last_30_days\"\n",
    "]\n",
    "categorical_features = [\"location\"]\n",
    "\n",
    "# Handle potential nulls in numerical features\n",
    "for col_name in numerical_features:\n",
    "    customer_features_df = customer_features_df.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
    "    customer_features_df = customer_features_df.fillna(0.0, subset=[col_name])\n",
    "\n",
    "# Handle potential nulls in categorical features\n",
    "for col_name in categorical_features:\n",
    "    customer_features_df = customer_features_df.fillna(\"Unknown\", subset=[col_name])\n",
    "\n",
    "\n",
    "# Create a Pipeline for feature processing\n",
    "# StringIndexer for categorical features\n",
    "indexers = []\n",
    "for col_name in categorical_features:\n",
    "    indexer = StringIndexer()\n",
    "    indexer.setInputCol(col_name)\n",
    "    indexer.setOutputCol(col_name + \"_indexed\")\n",
    "    indexer.setHandleInvalid(\"keep\") \n",
    "    indexers.append(indexer)\n",
    "\n",
    "# OneHotEncoder for indexed categorical features\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=indexer.getOutputCol() + \"_encoded\")\n",
    "    for indexer in indexers\n",
    "]\n",
    "\n",
    "# Assemble all features into a single vector column\n",
    "assembler_inputs = numerical_features + [encoder.getOutputCol() for encoder in encoders]\n",
    "vector_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\", handleInvalid=\"keep\")\n",
    "\n",
    "# Create a Pipeline for feature transformation\n",
    "feature_pipeline = Pipeline(stages=indexers + encoders + [vector_assembler])\n",
    "\n",
    "# Fit the feature pipeline to the data and transform it\n",
    "ml_data_credit_risk_regression = feature_pipeline.fit(customer_features_df).transform(customer_features_df)\n",
    "\n",
    "# Filter out rows where credit_score is null\n",
    "ml_data_credit_risk_regression = ml_data_credit_risk_regression.filter(col(\"label\").isNotNull())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "620f5af5-bdac-4de2-bd2f-7311be9a6613",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train and Evaluate Machine Learning Regression Model\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_data_cr, test_data_cr = ml_data_credit_risk_regression.randomSplit([0.7, 0.3], seed=42)\n",
    "print(f\"\\nCredit Risk Regression Training data count: {train_data_cr.count()}\")\n",
    "print(f\"Credit Risk Regression Test data count: {test_data_cr.count()}\")\n",
    "\n",
    "# Initialize Linear Regression model\n",
    "lr_reg = LinearRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10, regParam=0.1, elasticNetParam=0.0)\n",
    "\n",
    "# Train the model\n",
    "print(\"\\n--- Training Linear Regression Model for Credit Risk ---\")\n",
    "lr_reg_model = lr_reg.fit(train_data_cr)\n",
    "print(\"Credit Risk Regression Model training complete.\")\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_cr_reg = lr_reg_model.transform(test_data_cr)\n",
    "\n",
    "print(\"\\n--- Predictions on Credit Risk Test Data (first 10 rows) ---\")\n",
    "predictions_cr_reg.select(\"customer_id\", \"label\", \"prediction\").show(10, truncate=False)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator_rmse = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"rmse\")\n",
    "rmse = evaluator_rmse.evaluate(predictions_cr_reg)\n",
    "print(f\"\\nCredit Risk Model RMSE on test data: {rmse}\")\n",
    "\n",
    "evaluator_r2 = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"r2\")\n",
    "r2 = evaluator_r2.evaluate(predictions_cr_reg)\n",
    "print(f\"Credit Risk Model R-squared on test data: {r2}\")\n",
    "\n",
    "final_predictions_cr_reg_df = lr_reg_model.transform(ml_data_credit_risk_regression)\n",
    "\n",
    "# Select relevant columns \n",
    "ranked_customers_df = final_predictions_cr_reg_df.select(\n",
    "    col(\"customer_id\"),\n",
    "    col(\"age\"),\n",
    "    col(\"location\"),\n",
    "    col(\"annual_income\"),\n",
    "    col(\"dti\"),\n",
    "    col(\"ltv\"),\n",
    "    col(\"label\").alias(\"actual_credit_score\"), # The actual credit score from the table\n",
    "    col(\"prediction\").alias(\"predicted_credit_score\") # The predicted credit score\n",
    ").orderBy(col(\"predicted_credit_score\").asc()) first\n",
    "\n",
    "print(\"\\n--- Customers Ranked from Highest Risk (Lowest Predicted Credit Score) to Lowest Risk ---\")\n",
    "ranked_customers_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6bea3b8-b623-4f7f-a016-81b126fefe04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ranked_customers_output_table_name = \"customers_ranked_by_credit_risk\"\n",
    "ranked_customers_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{database_name}.{ranked_customers_output_table_name}\")\n",
    "\n",
    "print(f\"\\nCustomers ranked by credit risk saved to '{database_name}.{ranked_customers_output_table_name}'.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "creditRiskScoreModel",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
