{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e2cf426-a94f-4840-9d67-b842e01dacd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Regression model to predict credit risk scores\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, when, lit, avg, count, sum\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# --- 1. Define Database and Table Names, and Read Existing Data ---\n",
    "# IMPORTANT: Ensure these database and table names match your actual setup in Databricks.\n",
    "# The 'customers' and 'transactions' tables are assumed to already exist with the specified schema.\n",
    "\n",
    "database_name = \"banking_database\" \n",
    "customer_table_name = \"customers\"\n",
    "transactions_table_name = \"transactions\"\n",
    "\n",
    "# Read existing Customer Data from the Delta Table\n",
    "try:\n",
    "    # Expected Customer Table Schema:\n",
    "    # - Id (LongType)\n",
    "    # - First Name (StringType)\n",
    "    # - Last Name (StringType)\n",
    "    # - Age (IntegerType)\n",
    "    # - Location (StringType)\n",
    "    # - Annual Income (DoubleType)\n",
    "    # - Debt-To-Income Ratio (DTI) (DoubleType)\n",
    "    # - Loan-to-Value Ratio (LTV) (DoubleType)\n",
    "    # - Average Monthly Spending (DoubleType)\n",
    "    # - Credit Score (IntegerType)\n",
    "    spark_customer_df = spark.read.format(\"delta\").table(f\"{database_name}.{customer_table_name}\")\n",
    "    print(f\"Customer table '{database_name}.{customer_table_name}' loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading customer table: {e}\")\n",
    "    print(\"Please ensure the database and table exist and are accessible with the correct schema.\")\n",
    "    raise # Re-raise the exception to stop execution if tables are not found\n",
    "\n",
    "# Read existing Transactions Data from the Delta Table (optional for this model, but loaded for completeness)\n",
    "try:\n",
    "    # Expected Transactions Table Schema:\n",
    "    # - Customer ID (LongType)\n",
    "    # - Transaction Date (TimestampType)\n",
    "    # - Amount (DoubleType)\n",
    "    # - Recipient (StringType)\n",
    "    # - Device Type (StringType)\n",
    "    spark_transactions_df = spark.read.format(\"delta\").table(f\"{database_name}.{transactions_table_name}\")\n",
    "    print(f\"Transactions table '{database_name}.{transactions_table_name}' loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading transactions table: {e}\")\n",
    "    print(\"Please ensure the database and table exist and are accessible with the correct schema.\")\n",
    "    # Not critical to stop execution if transactions table is missing for this specific model,\n",
    "    # but good practice to catch.\n",
    "    pass\n",
    "\n",
    "print(\"\\n--- Raw Customer Data (first 5 rows) ---\")\n",
    "spark_customer_df.show(5)\n",
    "\n",
    "# --- 2. Feature Engineering and Label Creation for Credit Risk Scoring ---\n",
    "# For credit risk scoring, we primarily focus on customer-level attributes.\n",
    "# We'll create a dummy 'label' for credit risk. In a real scenario, this would be\n",
    "# based on historical loan default data or similar risk indicators.\n",
    "\n",
    "# Define rules for a dummy 'label' (1.0 for high risk, 0.0 for low risk)\n",
    "# These rules are illustrative and should be replaced with actual risk criteria.\n",
    "# Example rules:\n",
    "# - Credit Score below 600\n",
    "# - DTI (Debt-To-Income Ratio) above 0.45 (45%)\n",
    "# - LTV (Loan-to-Value Ratio) above 0.90 (90%)\n",
    "# - Combination of low credit score and high DTI/LTV\n",
    "\n",
    "customer_data_for_ml_df = spark_customer_df.withColumn(\n",
    "    \"label\", # 'label' is the standard column name for target variable in Spark ML\n",
    "    when(col(\"Credit Score\") < 600, lit(1.0)) # Low credit score\n",
    "    .when(col(\"Debt-To-Income Ratio (DTI)\") > 0.45, lit(1.0)) # High DTI\n",
    "    .when(col(\"Loan-to-Value Ratio (LTV)\") > 0.90, lit(1.0)) # High LTV\n",
    "    .when((col(\"Credit Score\") < 650) & (col(\"Debt-To-Income Ratio (DTI)\") > 0.40), lit(1.0)) # Combination\n",
    "    .otherwise(lit(0.0)) # Otherwise low risk\n",
    ")\n",
    "\n",
    "print(\"\\n--- Customer Data with Dummy 'label' for ML Training ---\")\n",
    "customer_data_for_ml_df.select(\n",
    "    \"Id\", \"Credit Score\", \"Debt-To-Income Ratio (DTI)\", \"Loan-to-Value Ratio (LTV)\", \"label\"\n",
    ").show(truncate=False)\n",
    "\n",
    "# Define numerical and categorical features for the credit risk model\n",
    "numerical_features = [\n",
    "    \"Age\", \"Annual Income\", \"Debt-To-Income Ratio (DTI)\", \"Loan-to-Value Ratio (LTV)\",\n",
    "    \"Average Monthly Spending\", \"Credit Score\"\n",
    "]\n",
    "categorical_features = [\"Location\"]\n",
    "\n",
    "# Create a Pipeline for feature processing\n",
    "# StringIndexer for categorical features\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=col_name, outputCol=col_name + \"_indexed\", handleInvalid=\"keep\")\n",
    "    for col_name in categorical_features\n",
    "]\n",
    "\n",
    "# OneHotEncoder for indexed categorical features\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=indexer.getOutputCol() + \"_encoded\")\n",
    "    for indexer in indexers\n",
    "]\n",
    "\n",
    "# Assemble all features into a single vector column\n",
    "assembler_inputs = numerical_features + [encoder.getOutputCol() for encoder in encoders]\n",
    "vector_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\", handleInvalid=\"keep\")\n",
    "\n",
    "# Create a Pipeline for feature transformation\n",
    "feature_pipeline = Pipeline(stages=indexers + encoders + [vector_assembler])\n",
    "\n",
    "# Fit the feature pipeline to the data and transform it\n",
    "# This step creates the 'features' column required by ML models\n",
    "ml_data_credit_risk = feature_pipeline.fit(customer_data_for_ml_df).transform(customer_data_for_ml_df)\n",
    "\n",
    "print(\"\\n--- ML Data for Credit Risk with 'features' Vector Column (first 5 rows) ---\")\n",
    "ml_data_credit_risk.select(\"Id\", \"Credit Score\", \"label\", \"features\").show(5, truncate=False)\n",
    "\n",
    "# --- 3. Train and Evaluate Machine Learning Model ---\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_data_cr, test_data_cr = ml_data_credit_risk.randomSplit([0.7, 0.3], seed=42)\n",
    "print(f\"\\nCredit Risk Training data count: {train_data_cr.count()}\")\n",
    "print(f\"Credit Risk Test data count: {test_data_cr.count()}\")\n",
    "\n",
    "# Initialize Logistic Regression model for credit risk\n",
    "lr_cr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Train the model\n",
    "print(\"\\n--- Training Logistic Regression Model for Credit Risk ---\")\n",
    "lr_credit_risk_model = lr_cr.fit(train_data_cr)\n",
    "print(\"Credit Risk Model training complete.\")\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_cr = lr_credit_risk_model.transform(test_data_cr)\n",
    "\n",
    "print(\"\\n--- Predictions on Credit Risk Test Data (first 10 rows) ---\")\n",
    "predictions_cr.select(\"Id\", \"Credit Score\", \"label\", \"prediction\", \"probability\").show(10, truncate=False)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator_cr = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "auc_cr = evaluator_cr.evaluate(predictions_cr)\n",
    "print(f\"\\nCredit Risk Model Area Under ROC (AUC) on test data: {auc_cr}\")\n",
    "\n",
    "evaluator_accuracy_cr = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"accuracy\")\n",
    "accuracy_cr = evaluator_accuracy_cr.evaluate(predictions_cr)\n",
    "print(f\"Credit Risk Model Accuracy on test data: {accuracy_cr}\")\n",
    "\n",
    "# --- 4. Apply the Trained Model for Credit Risk Prediction ---\n",
    "# Apply the trained model to the entire customer dataset (or new incoming customer data)\n",
    "\n",
    "final_predictions_cr_df = lr_credit_risk_model.transform(ml_data_credit_risk)\n",
    "\n",
    "# Select relevant columns and rename 'prediction' to 'predicted_credit_risk' for clarity\n",
    "credit_risk_results_df = final_predictions_cr_df.select(\n",
    "    col(\"Id\").alias(\"customer_id\"),\n",
    "    col(\"First Name\"),\n",
    "    col(\"Last Name\"),\n",
    "    col(\"Credit Score\"),\n",
    "    col(\"Debt-To-Income Ratio (DTI)\"),\n",
    "    col(\"Loan-to-Value Ratio (LTV)\"),\n",
    "    col(\"label\").alias(\"actual_risk_label\"), # The dummy label used for training\n",
    "    col(\"prediction\").cast(\"boolean\").alias(\"is_high_credit_risk_predicted\"), # Convert 0.0/1.0 to boolean\n",
    "    col(\"probability\")[1].alias(\"high_risk_probability\") # Probability of being class 1 (high risk)\n",
    ")\n",
    "\n",
    "print(\"\\n--- Final Credit Risk Predictions on All Customer Data ---\")\n",
    "credit_risk_results_df.orderBy(col(\"high_risk_probability\").desc()).show(truncate=False)\n",
    "\n",
    "print(\"\\n--- Customers Predicted as High Credit Risk ---\")\n",
    "credit_risk_results_df.filter(col(\"is_high_credit_risk_predicted\") == True).orderBy(col(\"high_risk_probability\").desc()).show(truncate=False)\n",
    "\n",
    "# --- Optional: Save High Credit Risk Customers to a separate table ---\n",
    "credit_risk_output_table_name = \"high_credit_risk_customers\"\n",
    "credit_risk_results_df.filter(col(\"is_high_credit_risk_predicted\") == True) \\\n",
    "    .write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{database_name}.{credit_risk_output_table_name}\")\n",
    "\n",
    "print(f\"\\nHigh credit risk customers saved to '{database_name}.{credit_risk_output_table_name}'.\")\n",
    "\n",
    "print(\"\\n--- Verify Credit Risk Output Table ---\")\n",
    "spark.sql(f\"SELECT * FROM {database_name}.{credit_risk_output_table_name}\").show(truncate=False)\n",
    "\n",
    "# --- Clean up temporary views (optional) ---\n",
    "# Note: The temporary views from the fraud model are not directly used here,\n",
    "# but it's good practice to drop them if they were created in the same session.\n",
    "spark.sql(\"DROP VIEW IF EXISTS all_customers\")\n",
    "spark.sql(\"DROP VIEW IF EXISTS all_transactions\")\n",
    "print(\"\\nTemporary views dropped.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "creditRiskScoreModel",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
